# Publishing Network Analyzer

This is my short generator for research collaboration networks. The aim of this tool is to create an effective report - including visuals and data anslysis - on the state of union or cohesiveness of a set of researchers. 

## Historical context

Originally, this tool was built to create supporting data to analyze the success of the Grand Challenges research program of ![UBC's SBQMI](https://qmi.ubc.ca/), the own goal of this program being to develop more cohesion in the institute. Reports generated via this tool enable the verification of to which extent the research output generated by the program represented "more than the sum" of that of the individual PI membership of the institute.

## Way of use

This tool is written in Python. To use it, install Python, clone this repo, and run `python research_network_analyzer.py`. I also provide a Jupyter notebook version of it, in case you prefer to edit information more dynamically.

The script ingests data from three .csv files:

* researchers.csv - the names of the researchers in the network, listed in a single csv column
* coauthors.csv - a list of co-authorships organized by rows of pairs of researcher names for every publication they share as co-authors
* current_collaborators.csv - a similar list of pairs of researcher names for every collaboration in development with the program

Users should simply add names and pairs of names corresponding to researchers one desires to study according to the model files in this repo. 

The program will generate a plot of the network, plus two reports: one the state of the network without in-development projects (aka "current state"), and the other with the full data including the result of the network once in-development projects are done (aka "target state"). The reports are displayed both in a .txt file with the data, and in graphic form.

## Data analysis and reporting

The data analysis and report use a series of quantities from graph theory to study the different features of your network, like its connectivity and robustness. Here are the quantities we generate, with a brief description of their meaning for your data:

Degree Distribution: the distribution of degrees (number of connections) can provide insights into the connectivity of the network. In an academic network, we are looking at the number of collaborations each professor has.

Centrality Measures: Centrality measures identify the most important nodes in the network. We use here the following measures of centrality:

* Degree Centrality: the number of connections each node has.
* Betweenness Centrality: the number of shortest paths that pass through a node, indicating its importance as a bridge between different parts of the network.
* Closeness Centrality: Measures how close a node is to all other nodes in the network, indicating its ability to quickly communicate or spread information.
* Clustering Coefficient: the degree to which nodes tend to cluster together. In our research network, this indicates the presence of tightly-knit research groups.

Path Length Distribution: the distribution of shortest path lengths between nodes provides insights into the overall structure and connectivity of the network.

Network Robustness: assesses how the network responds to node or edge removal, providing insights into its resilience.

## Room for improvement

As is, this tool requires that you bring .csv files with the data you want to study. Because different fields of science, different research groups, or institutes, all use different methods to track their data (SQL data bases, bibtex, etc.), I opted to leave users to generate the data on their own. If you have IT support, that can be quite easy, but if not, you may have to populate the data files by hand (ok for simple networks, not so fun for complicated ones). 

I am interested in creating general tools to scrap data bases and collect information for users of this tool. Will be happy with any contribution along these lines.